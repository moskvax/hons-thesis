\chapter{Experimental Setup} \label{chap:experiments}

In this chapter we detail the steps carried out as part of our investigation.
We first outline the nature of the data set, describe how it was cleaned and
pre-processed before it was used to train various classifiers. The methods of
evaluation are then presented, as these are critical to assessing the
performance of the models created.

\section{Trauma patient data set}

\subsection{Collection}
The data set used consisted of trauma registry data from the trauma centre
at the Royal Prince Alfred Hospital, a major trauma centre in New South Wales,
Australia. It covered all adult (age 15 and over) inpatient admissions to the
trauma centre from 2007--2011. 

All patients were first admitted to the trauma ward until discharged
or transferred to an appropriate unit within the hospital. A single trained
data manager recorded a variety of attributes about the admitted patient,
such as age, gender, blood pressure, mechanism of injury and body regions
that were injured. We received this data in a Microsoft Excel spreadsheet.

\subsection{Characteristics}
There were 2546 patient records in the data set we received, comprising of 74
features, one of which was the target variable \textbf{LOS48}. LOS48 was a
binary variable -- that is, it could only take two values, 0 or 1 -- with 1
indicating that the patient stayed two days or less, and 0 indicating a stay
of greater than 2 days.

\todo{Perhaps insert some more information about the data itself, such
as the various attributes and their data types? Not sure if that should go
here, in the results, or in an appendix -- it is relevant but can disturb
the flow of the main text.}

\section{Data preparation}
In any data mining task, adequately preparing the data ensures the best
chance of success: this includes deciding how to deal with missing and
inaccurate values, as well as appropriately transforming the data
\citep{Witten2005} and selecting or constructing the most suitable features
for the task \citep{Kotsiantis2006}.
In this section we describe how we addressed each of
the major issues and the reasons for our choice.

\subsection{Cleaning and normalisation} % missing values, irrelevant features
Since the data was initially in an Excel spreadsheet, we explored the data
by hand using Microsoft Excel's filter function on each column and
consolidated many categorical values simply
by correcting spelling and punctuation. For example:
\begin{itemize}
  \item In the \textbf{sex} attribute, there were values of `Female' and
  `Femal' which were consolidated into `Female'.
  \item In the \textbf{mechanism} attribute, which was textual and indicated
  the category of how the patient was injured, there were such categories as
  `Other Vehi' and `Other Vehicle' as well as `Pedal Cycl' and `Pedal Cyclist'.
  Such categories were combined into `Other Vehicle' and `Pedal Cyclist'
  respectively.
  \item Some categories in \textbf{mechanism} were also divided into whether or
  not the mechanism of injury was self-inflicted, such as `Shooting' and
  `Shooting-selfinflicted'. When we asked the domain expert, Michael Dinh, his
  suggestion was to combine them into one category: in this case, simply
  `Shooting'; this was done for all the categories in this attribute.
  \item \textbf{disp from ED} indicated the section of the hospital that the
  patient was discharged to. This was also a textual categorical attribute.
  There were redundancies like `G. ICU' and `G.ICU' and even `GICU', which we
  all combined into one category, `GICU'.
\end{itemize}

The data was then exported into CSV (comma-separated value) format, so that
we could continue to process it programmatically rather than manually. Many
fields in the data set contained leading or trailing whitespace, which was
removed with a few lines of Python code (see appendix \todo{[add reference
to appendix]}). All attribute names were converted to lowercase. In the case
where attribute names consisted of several words, the spaces were replaced
with underscores (\_) to ensure that the full attribute name was read.
\todo{Include a list of these converted attribute names? In the appendix?}

In order for our results to be comparable to that of Dinh
et. al. \citep{Dinh2013a}, we removed all instances with missing values,
leaving 2517 instances with which to use to create our prediction models.

\subsection{Feature selection}
After data preparation, selecting a subset of features or attributes to use
in a model is also important, and helps to reduce noise and redundant
attributes from becoming part of the final model. Experiments have found that
the performance of various classification methods such as decision trees
\textit{deteriorates} in the presence of useless attributes.
Our data set contains 73
non-target attributes, which is a fairly high number. Many of these attributes
are derived from others in the set, such as \textbf{age} and \textbf{age65}.

We began the process of feature selection by removing attributes that were
not available upon admission, as these tended to be highly related to the
LOS48 outcome and were a consequence, not a predictor, of LOS. These attributes
were: iculos, outcome, disp from ED, disp from hosp, died, rehab and los.
The id attribute was also removed because it simply numbered each record and
indicated nothing about the patient themself. This left us with 67 attributes.

The work of actually selecting the attributes out of the 67 available was
carried out in two ways: automated using feature selection methods, and
manually using advice from a domain expert.

\subsubsection{Automated feature selection methods}
There are a number of methods that are commonly used to automate the selection
of a subset of features for the final classifier. The methods we used were:
\begin{itemize}
  \item \textit{Correlation-based feature selection} \citep{Hall2000},
  which evaluates the `merit' of a
  subset of features by considering their degree of correlation with the target
  variable compared to their degree of correlation amongst each other. High
  correlation between variables indicates redundancy. Under
  this scheme, a set of features that are highly correlated with the target
  while having low correlation with other features in the set is ideal.
  \item Selection based on the
  \textit{Pearson product-moment correlation coefficient}, a measure of the
  linear correlation or dependence between two variables.\todo{add some more info}
  \item \textit{Information gain} \todo{explain a bit about this}
  \item Feature selection based on the \textit{OneR classifier}
  \citep{Holte1993}, which uses one
  attribute as the predictor for the target. This attribute is typically the one
  that has the lowest prediction error. Feature selection using this scheme
  involves first ranking attributes based on their error, with the lowest error
  attributes as the most desirable. We then selected the top 20 attributes to
  use in the building of the model.
\end{itemize}

We used the WEKA 3.7.11 \citep{Hall2009} implementation of the above
methods, after converting the data into the Attribute-Relation File Format
(ARFF)\footnote{\url{http://weka.wikispaces.com/ARFF+\%28developer+version\%29}}
most suitable for the program.
\todo{give more details, ie specifying binary/numeric
attributes, maybe specify in appendix?}

\subsubsection{Expert selection}
We also wanted to compare the feature selection of an expert in the domain
of the data set, namely trauma, with the models obtained from the features
selected using the automated methods described above. This resulted in two
feature subsets. The first was obtained simply by presenting the entire list
of 73 features to the domain expert and asking him to select the features he
thought were most indicative of the target variable from experience. The
second set was obtained from the work of Dinh et. al. \citep{Dinh2013a} that
we are building on, giving us a baseline to compare our results to.

Altogether, the various feature selection methods resulted in a total of
7 data sets (4 automated, 2 expert selected, and 1 with all features) which
we used in the model construction phase.

\section{Model construction}
Here we present the various classification schemes we used in order to find
the best model for the trauma LOS data.

\subsection{Single classifier}
First we sought to compare the suitability and performance of various
classifiers by themselves (later we looked at \textit{ensembles} of
classifiers, when more than one is involved in making a prediction).
These are the
classifiers studied: \todo{expand a bit more here, maybe list the
configuration parameters}
\begin{itemize}
  \item Na\"{i}ve Bayes
  \item Decision tree (C4.5)
  \item Logistic regression
  \item Support vector machine
  \item $K$-nearest neighbour
  \item Feed-forward neural network (multilayer perceptron)
\end{itemize}

Each classifier was trained using ten-fold cross-validation for each of the
7 data sets obtained from the various feature selection methods described
above. This was repeated 30 times for each classifier and the reasons for
this, as well as the metrics used to assess the performance of these
classifiers, are described in Section \ref{sec:perfeval} of this chapter.

In addition to the feature selection methods, we also used the WEKA 3.7.11
\citep{Hall2009} implementation of the above classifiers for our experiments.
Training was automated using a Bash shell script (whose source can be found
in the appendix) that programmatically read in a list of classifiers to
train and repeated the procedure any specified number of times, writing the
results to a specified directory for later reference.

\subsubsection{Parameter tuning}
It is difficult in data mining to apply an algorithm out-of-the-box and have
it perform optimally on a given data set \citep{Witten2005}. Therefore, time
must be spent tweaking and tuning the parameters of a classifier in order to
improve classification performance. This was the next step we took. We
considered a range of values for various parameters of each classification
scheme, and tried some of these new configurations by hand and some by using
WEKA's cross-validation parameter selection class. The configurations tested,
as well as how they were executed, are listed in this table:
\todo{insert table!}

\subsection{Ensemble classifiers}
In order to improve upon the performance of individual classifiers, we also
tested the performance of ensembles of classifiers -- that is, combining the
predictions of several different classifiers, or several of the same
classifier. Specifically, the ensemble methods used were:
\begin{itemize}
  \item Random Forests
  \item Bagging
  \item Stacked generalisation (stacking)
  \item Boosting (AdaBoostM1)
\end{itemize}

Again, we were able to create predictive models using the implementations of
these ensemble methods in WEKA 3.7.11. Since ensemble classifiers are not
actual classifiers but a way to train and combine the predictive power of more
than one classifier, we list the combinations of classifiers used in our
ensemble methods:
\todo{insert table!}

\section{Performance evaluation} % don't forget cross-validation
\label{sec:perfeval}
Here we describe the methods used in order to assess the performance of the
models constructed through single classifiers and ensemble methods described
above.

\subsection{Metrics}
There are many available and commonly used metrics that we can use when
evaluating and comparing the performance of prediction models. The ones that
we use are: \todo{maybe put these in a table}
\begin{itemize}
  \item Accuracy
  \item Specificity
  \item Sensitivity
  \item Area under the receiver-operating characteristic (ROC) curve, or AUC
\end{itemize}

Since we only have one data set, we use ten-fold stratified cross-validation
to obtain values for the above metrics. We run the ten-fold cross-validation
40 times for each classifier (single or ensemble) for each of the 7 data sets
produced from feature selection. 40 runs was necessary in order for us to
compute confidence intervals for each of the metrics, which makes use of the
Central Limit Theorem from statistics.

\subsubsection{Confidence intervals}
In order to compare our results with that of Dinh et. al. \citep{Dinh2013a},
we are especially interested in the mean cross-validated AUC of our
classifiers as well as
the upper and lower bounds of the 95\% confidence interval associated with this
mean. The intervals are constructed using this argument:

For each classifier, let the population of all of its ten-fold cross-validated
AUC scores be some random variable $X$, which follows a distribution whose
parameters (such as mean and standard deviation) we do not know.
The Central Limit Theorem states that given a random
sample of $n$ items from $X$, namely $X_1,X_2,\dots,X_n$, the sample mean
$\bar{x}$
is approximately normally distributed when $n$ is at least roughly around 30
and the $X_i$'s are independent and identically distributed.
Consequently, we can construct a $(1-\alpha)$\% confidence interval for
$\bar{x}$ -- where $\alpha$ is commonly called the
\textit{significance level} -- as follows:
\begin{equation}
  \left[\bar{x} - z_{1-\alpha/2}\dfrac{s}{\sqrt{n}},
    \bar{x} + z_{1-\alpha/2}\dfrac{s}{\sqrt{n}}\right]
\end{equation}
where $\bar{x}$ is the sample mean, $\alpha$ is the significance level
(5\% or $0.05$ in our case), $s$ is the sample standard deviation, $n$
is the number of observations in our sample, and $z_{1-\alpha/2}$ is the value
of the standard normal variable $Z \sim \mathcal{N}(0,1)$ with cumulative
probability $1-\alpha/2$. Since we have taken 40 independent samples of the
ten-fold cross-validated AUC, the Central Limit Theorem holds and we can
compute the mean and standard deviation of our 40 samples in order to
construct a 95\% confidence interval for the mean AUC of each classifier.

\subsection{Statistical tests}
To decide whether or not the difference in values of each metric between
classifiers was statistically significant, paired $t$-tests were conducted for
a subset of the results: \todo{elaborate}
\begin{itemize}
  \item Some feature set vs other feature set
  \item Single classifier vs other single classifier
  \item Ensemble vs single
  \item Our best method vs Dinh2013's method
\end{itemize}
